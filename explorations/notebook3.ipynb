{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Adagrad\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape:  (9349, 3)\n",
      "train dataset shape:  (493, 2)\n",
      "train columns:  Index(['gold_label', 'sentence1', 'sentence2'], dtype='object')\n",
      "test columns:  Index(['sentence1', 'sentence2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "training_dataset = pd.read_csv('dataset/train.csv')\n",
    "testing_dataset = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "print('train dataset shape: ', training_dataset.shape)\n",
    "print('train dataset shape: ', testing_dataset.shape)\n",
    "print('train columns: ', training_dataset.columns)\n",
    "print('test columns: ', testing_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sents(sentence):\n",
    "    return re.sub('[^A-Za-z\\-]+', ' ', str(sentence)).replace(\"'\", '').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = training_dataset.copy()\n",
    "train_df.sentence1 = train_df.sentence1.apply(clean_sents)\n",
    "train_df.sentence2 = train_df.sentence2.apply(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['gold_label'], axis=1)\n",
    "y = train_df[['gold_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 1, 0, 2, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded, y_categories = y['gold_label'].factorize()\n",
    "y_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['contradiction', 'entailment', 'neutral'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y_encoded, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 100000\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors/GoogleNews-vectors-negative300.bin', \n",
    "                                              binary=True, limit=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = w2v_model.vectors[:max_vocab_size, :]\n",
    "embeddings = np.concatenate((np.zeros((1,300)), embeddings))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: i+1 for i, word in enumerate(w2v_model.index2word) if i < max_vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('word index: {}'.format(word2index['man']))\n",
    "# print('word vector: ', embeddings[word2index['man']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embeddings(sentence, word2index):\n",
    "    return np.array([word2index[wrd] if wrd in word2index else 0 for wrd in sentence.split(' ')])\n",
    "\n",
    "X['x1'] = X.sentence1.apply(lambda x: words_to_embeddings(x, word2index))\n",
    "X['x2'] = X.sentence2.apply(lambda x: words_to_embeddings(x, word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=29)\n",
    "\n",
    "for train_index, test_index in sss.split(X.drop(['sentence1','sentence2'],axis=1), y):\n",
    "    X_train = X.drop(['sentence1','sentence2'],axis=1).loc[train_index]\n",
    "    X_test = X.drop(['sentence1','sentence2'],axis=1).loc[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>[0, 534, 252, 2362, 0, 988, 5872, 0, 988, 7946...</td>\n",
       "      <td>[73, 20, 308, 3052, 421, 4, 252, 2, 988, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9147</th>\n",
       "      <td>[0, 5268, 391, 380, 70, 0, 6562, 0]</td>\n",
       "      <td>[12, 642, 5, 5268, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     x1  \\\n",
       "5857  [0, 534, 252, 2362, 0, 988, 5872, 0, 988, 7946...   \n",
       "9147                [0, 5268, 391, 380, 70, 0, 6562, 0]   \n",
       "\n",
       "                                               x2  \n",
       "5857  [73, 20, 308, 3052, 421, 4, 252, 2, 988, 0]  \n",
       "9147                        [12, 642, 5, 5268, 0]  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 49)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(X_train.x1.map(len)), max(X_train.x2.map(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max( max(X_train.x1.map(len)), max(X_train.x2.map(len)) )\n",
    "\n",
    "x1_padded = pad_sequences(X_train.x1, maxlen=max_seq_len)\n",
    "x2_padded = pad_sequences(X_train.x2, maxlen=max_seq_len)\n",
    "\n",
    "x1_test_padded = pad_sequences(X_test.x1, maxlen=max_seq_len)\n",
    "x2_test_padded = pad_sequences(X_test.x2, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  1214,   435,     0],\n",
       "       [    0,     0,     0, ...,     2,  3127,     0],\n",
       "       [    0,     0,     0, ...,    13,     0, 13674],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     5,     0,     0],\n",
       "       [    0,     0,     0, ...,    28, 36566,     0],\n",
       "       [    0,     0,     0, ...,     6,    12,  1393]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = np.c_[x1_padded, x2_padded]\n",
    "test_set = np.c_[x1_test_padded, x2_test_padded]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     5,  2265,     0],\n",
       "       [    0,     0,     0, ..., 11240,  2458,     0],\n",
       "       [    0,     0,     0, ...,    20,   587,     0],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   581,  1002,     0],\n",
       "       [    0,     0,     0, ...,   224,    67,     0],\n",
       "       [    0,     0,     0, ...,   252,     5,  2026]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9349, 4)\n",
      "(7479, 102)\n",
      "(1870, 102)\n",
      "\n",
      "(9349, 3)\n",
      "(7479, 3)\n",
      "(1870, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "print()\n",
    "print(y.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/10\n",
      " - 70s - loss: 0.5960 - acc: 0.6879 - val_loss: 0.5563 - val_acc: 0.7087\n",
      "Epoch 2/10\n",
      " - 69s - loss: 0.4847 - acc: 0.7689 - val_loss: 0.5595 - val_acc: 0.7191\n",
      "Epoch 3/10\n",
      " - 73s - loss: 0.4042 - acc: 0.8155 - val_loss: 0.5990 - val_acc: 0.7157\n",
      "Epoch 4/10\n",
      " - 74s - loss: 0.3511 - acc: 0.8432 - val_loss: 0.6562 - val_acc: 0.7100\n",
      "Epoch 5/10\n",
      " - 74s - loss: 0.3108 - acc: 0.8634 - val_loss: 0.6898 - val_acc: 0.7080\n",
      "Epoch 6/10\n",
      " - 73s - loss: 0.2805 - acc: 0.8765 - val_loss: 0.7463 - val_acc: 0.7064\n",
      "Epoch 7/10\n",
      " - 72s - loss: 0.2563 - acc: 0.8906 - val_loss: 0.7904 - val_acc: 0.7005\n",
      "Epoch 8/10\n",
      " - 70s - loss: 0.2403 - acc: 0.8975 - val_loss: 0.8328 - val_acc: 0.7018\n",
      "Epoch 9/10\n",
      " - 68s - loss: 0.2213 - acc: 0.9044 - val_loss: 0.8869 - val_acc: 0.7018\n",
      "Epoch 10/10\n",
      " - 73s - loss: 0.2067 - acc: 0.9125 - val_loss: 0.9390 - val_acc: 0.6959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1357f1a50f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model.add(GRU(units=32, dropout=(0.2), recurrent_dropout=(0.2)))\n",
    "# model.add(Dense(3, activation='tanh')) #TODO: rem this, too much complexity\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, epsilon=1e-08)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_set, y_train, batch_size=32, epochs=10, validation_data=(test_set, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,032,367\n",
      "Trainable params: 30,032,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 98, 128)           192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 49, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               627300    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 30,820,031\n",
      "Trainable params: 30,820,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training...\n",
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/5\n",
      " - 76s - loss: 0.6173 - acc: 0.6754 - val_loss: 0.5894 - val_acc: 0.6938\n",
      "Epoch 2/5\n",
      " - 74s - loss: 0.5358 - acc: 0.7331 - val_loss: 0.5433 - val_acc: 0.7308\n",
      "Epoch 3/5\n",
      " - 74s - loss: 0.4034 - acc: 0.8179 - val_loss: 0.6104 - val_acc: 0.7150\n",
      "Epoch 4/5\n",
      " - 72s - loss: 0.2902 - acc: 0.8752 - val_loss: 0.7314 - val_acc: 0.7018\n",
      "Epoch 5/5\n",
      " - 73s - loss: 0.2077 - acc: 0.9155 - val_loss: 0.9262 - val_acc: 0.6822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13652c92828>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training...\")\n",
    "model2.fit(train_set, y_train, batch_size=32, epochs=5, validation_data=(test_set, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,032,367\n",
      "Trainable params: 30,032,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/6\n",
      " - 71s - loss: 0.6051 - acc: 0.6832 - val_loss: 0.5587 - val_acc: 0.7178\n",
      "Epoch 2/6\n",
      " - 68s - loss: 0.5006 - acc: 0.7601 - val_loss: 0.5630 - val_acc: 0.7212\n",
      "Epoch 3/6\n",
      " - 70s - loss: 0.4197 - acc: 0.8094 - val_loss: 0.5861 - val_acc: 0.7160\n",
      "Epoch 4/6\n",
      " - 68s - loss: 0.3697 - acc: 0.8405 - val_loss: 0.6422 - val_acc: 0.7125\n",
      "Epoch 5/6\n",
      " - 72s - loss: 0.3307 - acc: 0.8565 - val_loss: 0.6759 - val_acc: 0.7025\n",
      "Epoch 6/6\n",
      " - 70s - loss: 0.2996 - acc: 0.8698 - val_loss: 0.7509 - val_acc: 0.6979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1376065b748>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model3.add(GRU(units=32, dropout=(0.2), recurrent_dropout=(0.2)))\n",
    "model3.add(Dropout(rate=0.4))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "print(model3.summary())\n",
    "\n",
    "optimizer = Adam(lr=0.001, epsilon=1e-08)\n",
    "model3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model3.fit(train_set, y_train, batch_size=32, epochs=6, validation_data=(test_set, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,032,367\n",
      "Trainable params: 30,032,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/5\n",
      " - 64s - loss: 0.6182 - acc: 0.6730 - val_loss: 0.5849 - val_acc: 0.6982\n",
      "Epoch 2/5\n",
      " - 60s - loss: 0.5563 - acc: 0.7194 - val_loss: 0.5586 - val_acc: 0.7201\n",
      "Epoch 3/5\n",
      " - 56s - loss: 0.5117 - acc: 0.7526 - val_loss: 0.5575 - val_acc: 0.7166\n",
      "Epoch 4/5\n",
      " - 55s - loss: 0.4860 - acc: 0.7707 - val_loss: 0.5617 - val_acc: 0.7223\n",
      "Epoch 5/5\n",
      " - 55s - loss: 0.4645 - acc: 0.7860 - val_loss: 0.5714 - val_acc: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137668c67f0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model4.add(GRU(units=32, dropout=(0.4), recurrent_dropout=(0.4)))\n",
    "model4.add(Dropout(rate=0.4))\n",
    "model4.add(Dense(3, activation='softmax'))\n",
    "print(model4.summary())\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, epsilon=1e-08) #Adam(lr=0.001, epsilon=1e-08)\n",
    "model4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model4.fit(train_set, y_train, batch_size=32, epochs=5, validation_data=(test_set, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,032,367\n",
      "Trainable params: 30,032,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/10\n",
      "7479/7479 [==============================] - 63s 8ms/step - loss: 0.6176 - acc: 0.6732 - val_loss: 0.5850 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69661, saving model to model-epoch-01-val_acc-0.6966.hdf5\n",
      "Epoch 2/10\n",
      "7479/7479 [==============================] - 57s 8ms/step - loss: 0.5527 - acc: 0.7218 - val_loss: 0.5596 - val_acc: 0.7189\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69661 to 0.71889, saving model to model-epoch-02-val_acc-0.7189.hdf5\n",
      "Epoch 3/10\n",
      "7479/7479 [==============================] - 61s 8ms/step - loss: 0.5129 - acc: 0.7537 - val_loss: 0.5544 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71889 to 0.72246, saving model to model-epoch-03-val_acc-0.7225.hdf5\n",
      "Epoch 4/10\n",
      "7479/7479 [==============================] - 61s 8ms/step - loss: 0.4847 - acc: 0.7742 - val_loss: 0.5611 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72246\n",
      "Epoch 5/10\n",
      "7479/7479 [==============================] - 59s 8ms/step - loss: 0.4626 - acc: 0.7871 - val_loss: 0.5679 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72246 to 0.72424, saving model to model-epoch-05-val_acc-0.7242.hdf5\n",
      "Epoch 6/10\n",
      "7479/7479 [==============================] - 59s 8ms/step - loss: 0.4449 - acc: 0.7986 - val_loss: 0.5774 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72424\n",
      "Epoch 7/10\n",
      "7479/7479 [==============================] - 62s 8ms/step - loss: 0.4268 - acc: 0.8109 - val_loss: 0.5876 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.72424\n",
      "Epoch 8/10\n",
      "7479/7479 [==============================] - 61s 8ms/step - loss: 0.4145 - acc: 0.8162 - val_loss: 0.5955 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.72424\n",
      "Epoch 9/10\n",
      "7479/7479 [==============================] - 61s 8ms/step - loss: 0.4006 - acc: 0.8244 - val_loss: 0.6113 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.72424\n",
      "Epoch 10/10\n",
      "7479/7479 [==============================] - 68s 9ms/step - loss: 0.3929 - acc: 0.8322 - val_loss: 0.6149 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.72424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1376c95c978>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best so far\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model4.add(GRU(units=32, dropout=(0.4), recurrent_dropout=(0.4)))\n",
    "model4.add(Dropout(rate=0.4))\n",
    "model4.add(Dense(3, activation='softmax'))\n",
    "print(model4.summary())\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, epsilon=1e-08) #Adam(lr=0.001, epsilon=1e-08)\n",
    "model4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "filepath = \"model-epoch-{epoch:02d}-val_acc-{val_acc:.4f}.hdf5\"\n",
    "chk_point = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "callbacks_list = [chk_point]\n",
    "\n",
    "model4.fit(train_set, y_train, batch_size=32, epochs=10, validation_data=(test_set, y_test), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 102, 300)          30000300  \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 30600)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 500)               15300500  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 45,351,203\n",
      "Trainable params: 45,351,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7479 samples, validate on 1870 samples\n",
      "Epoch 1/5\n",
      "7479/7479 [==============================] - 76s 10ms/step - loss: 0.7573 - acc: 0.6058 - val_loss: 0.6165 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66827, saving model to model-5-epoch-01-val_acc-0.6683.hdf5\n",
      "Epoch 2/5\n",
      "7479/7479 [==============================] - 71s 10ms/step - loss: 0.6039 - acc: 0.6861 - val_loss: 0.6282 - val_acc: 0.6734\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66827 to 0.67344, saving model to model-5-epoch-02-val_acc-0.6734.hdf5\n",
      "Epoch 3/5\n",
      "7479/7479 [==============================] - 72s 10ms/step - loss: 0.4605 - acc: 0.7834 - val_loss: 0.6511 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.67344\n",
      "Epoch 4/5\n",
      "7479/7479 [==============================] - 71s 10ms/step - loss: 0.3428 - acc: 0.8523 - val_loss: 0.7804 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67344\n",
      "Epoch 5/5\n",
      "7479/7479 [==============================] - 74s 10ms/step - loss: 0.2504 - acc: 0.8977 - val_loss: 0.9610 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13773f40fd0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_seq_len*2))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(500, activation='tanh'))\n",
    "model5.add(Dropout(rate=0.4))\n",
    "model5.add(Dense(100, activation='tanh'))\n",
    "model5.add(Dropout(rate=0.25))\n",
    "model5.add(Dense(3, activation='softmax'))\n",
    "print(model5.summary())\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, epsilon=1e-08) #Adam(lr=0.001, epsilon=1e-08)\n",
    "model5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "filepath = \"model-5-epoch-{epoch:02d}-val_acc-{val_acc:.4f}.hdf5\"\n",
    "chk_point = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "callbacks_list = [chk_point]\n",
    "\n",
    "model5.fit(train_set, y_train, batch_size=32, epochs=5, \n",
    "           validation_data=(test_set, y_test), \n",
    "           callbacks=callbacks_list\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=29, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=29)\n",
    "\n",
    "rf_clf.fit(train_set, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = rf_clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.12858869762398603\n",
      "recall:  0.33580839402516727\n",
      "f1:  0.18527246920315146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"precision: \", precision_score(y_pred_val, y_test, average='macro'))\n",
    "print(\"recall: \", recall_score(y_pred_val, y_test, average='macro'))\n",
    "print(\"f1: \", f1_score(y_pred_val, y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(rf_clf, param_distributions = random_grid, \n",
    "                               n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 27.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=29, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(train_set, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
